{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from sf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfdata = pd.read_csv(\"sf_crime_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert date to actual date format. This might take a while!\n",
    "sfdata.Date = sfdata['Date'].apply(lambda x: pd.to_datetime(x, format='%m/%d/%Y'))\n",
    "sfdata.Time = sfdata['Time'].apply(lambda x: pd.to_datetime(x, format=\"%H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buckets(series, n):\n",
    "    # Takes a series and returns a series mapping each element to a\n",
    "    # one of n buckets.\n",
    "    mi, ma = series.min(), series.max()\n",
    "    buckets = np.linspace(mi, ma, n)\n",
    "    print buckets\n",
    "    def f(e):\n",
    "        for i, el in enumerate(buckets):\n",
    "            if e < el:\n",
    "                return i\n",
    "        return n - 1\n",
    "            \n",
    "    res = series.map(f)\n",
    "    return res\n",
    "\n",
    "def cleanColumns(data):\n",
    "    # Used to rename the columns in our data grame to their appropriate names.\n",
    "    # Also drops unnecessary columns.\n",
    "    data['Latitude'] = data['Y']\n",
    "    data['Longitude'] = data['X']\n",
    "    data['Type'] = data['Category']\n",
    "    \n",
    "    print data.columns\n",
    "    data = data.drop(['IncidntNum', 'Category', 'Descript', 'PdDistrict','Resolution','Address','X','Y', 'Location'], axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def createPartitions(data, n):\n",
    "    # Remove outliers from the latitude/longitude issues\n",
    "    # We know that the city lies between -130, -120 longitude\n",
    "    # We also know that the citiy lies between 37 and 40 degrees latitude\n",
    "    data = data[-120 > data['Longitude']][data['Longitude'] > (-130)]\n",
    "    data = data[data['Latitude'] > 37][data['Latitude'] < 40]\n",
    "    \n",
    "    # Each row is an occurrance of a single crime. \n",
    "    # Keep around the original data\n",
    "    data['Region'] =  n *  buckets(data['Latitude'], n) + buckets(data['Longitude'],n) + 1\n",
    "    \n",
    "    # Add in the types into the results.\n",
    "    mapping = {key: i for i,key in enumerate(data['Type'].unique())}\n",
    "    data['TypeIndex'] = data['Type'].map(mapping)\n",
    "\n",
    "    # Now we can add the crime counts. \n",
    "    data['CrimeCount'] = np.ones(len(data))\n",
    "    return data\n",
    "\n",
    "def extractDateFeatures(data):\n",
    "    # Creates a new data frame and returns it as copy with all the data that we're interested in\n",
    "    # Create map from week days to integers\n",
    "    DayOfWeek = {'Sunday': 1,\n",
    "                 'Monday': 2,\n",
    "                 'Tuesday': 3,\n",
    "                 'Wednesday': 4,\n",
    "                 'Thursday': 5,\n",
    "                 'Friday': 6,\n",
    "                 'Saturday': 7 }\n",
    "    data['DoW'] = data['DayOfWeek'].map(DayOfWeek)\n",
    "    data = data.drop('DayOfWeek', axis=1)\n",
    "    print \"Created Weeks\"\n",
    "    \n",
    "    # We assume that the Date column is already in datetime format\n",
    "    data['Month'] = data.Date.map(lambda x: x.month)\n",
    "    data['DoM'] = data.Date.map(lambda x: x.day)\n",
    "    data['Year'] = data.Date.map(lambda x: x.year) - data.Date.min().year\n",
    "    data['ToD'] = data.Time.map(lambda x: x.minute)\n",
    "    data['Time'] = data.Time.map(lambda x: x.value / 10**9) - data.Date.min().value / 10**9\n",
    "    \n",
    "    # We add an additional column that combines the month and the year into number of months since beginning\n",
    "    data['TimeFeature'] = data.ix[:, ['Year', 'Month']].apply(lambda s: 12*s[0] + s[1], axis=1)\n",
    "    \n",
    "    data = data.drop('Date', axis=1)\n",
    "    \n",
    "    print \"Created the time data features!\"\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extractDataFeatures(data, n):\n",
    "    # Given the input data read directly from the exported data \n",
    "    # (https://data.sfgov.org/Public-Safety/Map-Crime-Incidents-from-1-Jan-2003/gxxq-x39z)\n",
    "    # We convert it into the format specified as follows:\n",
    "    # We want the results to be a data frame with the following columns.:\n",
    "    # -> Latitude (float)\n",
    "    # -> Longtitude (float)\n",
    "    # -> Region (specifies which region this data point belongs to)\n",
    "    # -> DoW (1-7 results) (Day of the Week)\n",
    "    # -> Month (1-12) (Month of the Year)\n",
    "    # -> DoM (1-31) (Day of the Month)\n",
    "    # -> Year (0->max(year) - min(year)) \n",
    "    # -> ToD (Time of Day, specified as number of minutes since start of day)\n",
    "    # -> Time (int) : #minutes since earliest sample in the data set\n",
    "    # -> Type (string) : Described the type of crime\n",
    "    # -> TypeIndex (int): Index mapping uniquely each crime type to a value in [1...#crime types]\n",
    "    # -> CrimeCount (int) : The number of crimes in this area\n",
    "    \n",
    "    # Remove unnecessary columns and rename\n",
    "    cData = cleanColumns(data)\n",
    "    \n",
    "    # Created partitions. Note that this modifies the data by adding a REGION column.\n",
    "    partitionedData = createPartitions(cData, n)\n",
    "    \n",
    "    # Now we convert the data to the correct dates and clean the data!\n",
    "    finalData = extractDateFeatures(partitionedData)   \n",
    "    \n",
    "    # Return the results\n",
    "    return finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countCrime(d, region):\n",
    "        '''\n",
    "        Counts the crime in a given region, returning an array of size 144 with halucinated empty data \n",
    "        for non-existent crime periods.\n",
    "        '''\n",
    "        res = np.zeros(144)\n",
    "        for i in range(144):\n",
    "            try:\n",
    "                # print d.ix[region, i].CrimeCount\n",
    "                res[i] = d.ix[region, i].CrimeCount\n",
    "            except (KeyError, AttributeError, IndexError) as e:\n",
    "                pass\n",
    "    \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitTrainTest(D, year=None):\n",
    "    '''\n",
    "    Given a data frame with the specified data, we split into a training set and a test set.\n",
    "    The test data consists of us holding out a particular year from our training data.\n",
    "    '''\n",
    "    # We only want to keep some of the data\n",
    "    D = D.ix[:, ['Year', 'TimeFeature', 'CrimeCount', 'Region']]\n",
    "    \n",
    "    # First, we're going to seperate by region.\n",
    "    if year is None:\n",
    "        test = D[D['Year'] == D['Year'].max()]\n",
    "        train = D[D['Year'] != D['Year'].max()]\n",
    "    else:\n",
    "        test = D[D['Year'] == year]\n",
    "        train = D[D['Year'] != year]\n",
    "        \n",
    "    # Now we keep only a subset of the columns we want, which is TimeFeature and CrimeCount\n",
    "    train = train.drop('Year', axis=1)\n",
    "    test = test.drop('Year', axis=1)\n",
    "\n",
    "    print \"Finished Creating Testing Set\"\n",
    "    \n",
    "    # Now we groupby TimeFeature which consists of YearMonth string.\n",
    "    trainD, testD = train.groupby(['Region', 'TimeFeature']).aggregate(np.sum), test.groupby([ 'Region', 'TimeFeature']).aggregate(np.sum)\n",
    "    # return trainD\n",
    "    trainRes = {}\n",
    "    testRes = {}\n",
    "    for region in range(D.Region.max()):\n",
    "        # Training\n",
    "        trainRes[region] = np.zeros((144,2))\n",
    "        trainRes[region][:,0] = range(144)\n",
    "        # print trainRes[region]\n",
    "        trainRes[region][:,1] = countCrime(trainD, region)\n",
    "        \n",
    "        # Test \n",
    "        testRes[region] = np.zeros((144,2))\n",
    "        testRes[region][:,0] = range(144) \n",
    "        testRes[region][:,1] = countCrime(testD, region)\n",
    "    \n",
    "    return trainRes, trainRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'IncidntNum', u'Category', u'Descript', u'DayOfWeek', u'Date', u'Time', u'PdDistrict', u'Resolution', u'Address', u'X', u'Y', u'Location', u'Latitude', u'Longitude', u'Type'], dtype='object')\n",
      "[ 37.70787902  37.72040589  37.73293276  37.74545963  37.7579865\n",
      "  37.77051336  37.78304023  37.7955671   37.80809397  37.82062084]\n",
      "[-122.51364206 -122.49709858 -122.4805551  -122.46401161 -122.44746813\n",
      " -122.43092464 -122.41438116 -122.39783767 -122.38129419 -122.3647507 ]\n",
      "Created Weeks\n",
      "Created the time data features!\n"
     ]
    }
   ],
   "source": [
    "results = extractDataFeatures(sfdata, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotHistogram(results, n):\n",
    "    fig, ax = plt.subplots( nrows=1, ncols=1 )\n",
    "    ax.hist(results.Region, bins=range(n*n))\n",
    "    plt.xlabel(\"San Francisco Region\")\n",
    "    plt.ylabel(\"Total Incidents of Reported Incidents\")\n",
    "    plt.title(\"Crime Incidents in San Francisco when Divided into {}x{} grid.\".format(n,n))\n",
    "    plt.savefig(\"sf_n{}\".format(n))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotHistogram(results, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Creating Testing Set\n"
     ]
    }
   ],
   "source": [
    "train, test = splitTrainTest(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRMSE(x,y):\n",
    "    return np.sqrt(np.sum((y - x)**2) / len(y))\n",
    "\n",
    "def trainAndTest(train, test):\n",
    "    # Given a set of training and testing data, train a linear regression model, tests it, and then \n",
    "    # computes the RMSE of each region, returning the resulting dictionary of RMSEs for region, as well as\n",
    "    # a dictionary of predictions, and a dictionary of trained models\n",
    "    models = {}\n",
    "    RMSE = {}\n",
    "    predictions = {}\n",
    "    for region in train:\n",
    "        if region % 10 == 0:\n",
    "            print \"Training on region {}\".format(region)\n",
    "        model = LinearRegression()\n",
    "        x = train[region][:,0]\n",
    "        x = x.reshape((len(x),1))\n",
    "        y = train[region][:,1]\n",
    "        y = y.reshape((len(y),1))\n",
    "        model.fit(x,y)\n",
    "        xTest = test[region][:,0]\n",
    "        xTest = xTest.reshape((len(xTest),1))\n",
    "        yTest = test[region][:,1]\n",
    "        yTest = yTest.reshape((len(yTest),1))\n",
    "        preds = model.predict(xTest)\n",
    "        rmse = getRMSE(yTest, preds)\n",
    "        \n",
    "        # store the results\n",
    "        models[region] = model\n",
    "        RMSE[region] = rmse\n",
    "        predictions[region] = preds\n",
    "    \n",
    "    return models, RMSE, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on region 0\n",
      "Training on region 10\n",
      "Training on region 20\n",
      "Training on region 30\n",
      "Training on region 40\n",
      "Training on region 50\n",
      "Training on region 60\n",
      "Training on region 70\n",
      "Training on region 80\n",
      "Training on region 90\n"
     ]
    }
   ],
   "source": [
    "models, RMSE, predictions = trainAndTest(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.802226607258753"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(RMSE.values()) / len(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 37.70787902]\n",
      "[-122.51364206]\n",
      "Finished Creating Testing Set\n",
      "Training on region 0\n",
      "Training on region 10\n",
      "Training on region 20\n",
      "Training on region 30\n",
      "Training on region 40\n",
      "Training on region 50\n",
      "Training on region 60\n",
      "Training on region 70\n",
      "Training on region 80\n",
      "Training on region 90\n",
      "[ 37.70787902  37.82062084]\n",
      "[-122.51364206 -122.3647507 ]\n",
      "Finished Creating Testing Set\n",
      "Training on region 0\n",
      "Training on region 10\n",
      "Training on region 20\n",
      "Training on region 30\n",
      "Training on region 40\n",
      "Training on region 50\n",
      "Training on region 60\n",
      "Training on region 70\n",
      "Training on region 80\n",
      "Training on region 90\n",
      "[ 37.70787902  37.76424993  37.82062084]\n",
      "[-122.51364206 -122.43919638 -122.3647507 ]\n",
      "Finished Creating Testing Set\n",
      "Training on region 0\n",
      "Training on region 10\n",
      "Training on region 20\n",
      "Training on region 30\n",
      "Training on region 40\n",
      "Training on region 50\n",
      "Training on region 60\n",
      "Training on region 70\n",
      "Training on region 80\n",
      "Training on region 90\n",
      "[ 37.70787902  37.74545963  37.78304023  37.82062084]\n",
      "[-122.51364206 -122.46401161 -122.41438116 -122.3647507 ]\n",
      "Finished Creating Testing Set\n",
      "Training on region 0\n",
      "Training on region 10\n",
      "Training on region 20\n",
      "Training on region 30\n",
      "Training on region 40\n",
      "Training on region 50\n",
      "Training on region 60\n",
      "Training on region 70\n",
      "Training on region 80\n",
      "Training on region 90\n",
      "[ 37.70787902  37.73606448  37.76424993  37.79243538  37.82062084]\n",
      "[-122.51364206 -122.47641922 -122.43919638 -122.40197354 -122.3647507 ]\n",
      "Finished Creating Testing Set\n",
      "Training on region 0\n",
      "Training on region 10\n",
      "Training on region 20\n",
      "Training on region 30\n",
      "Training on region 40\n",
      "Training on region 50\n",
      "Training on region 60\n",
      "Training on region 70\n",
      "Training on region 80\n",
      "Training on region 90\n",
      "[ 37.70787902  37.73042739  37.75297575  37.77552411  37.79807247\n",
      "  37.82062084]\n",
      "[-122.51364206 -122.48386379 -122.45408552 -122.42430725 -122.39452898\n",
      " -122.3647507 ]\n",
      "Finished Creating Testing Set\n",
      "Training on region 0"
     ]
    }
   ],
   "source": [
    "# We try different values of n and calculate the average RMSE for each value!\n",
    "# Note that results already has the data extracted\n",
    "rmses = []\n",
    "max_rmses = []\n",
    "min_rmses = []\n",
    "for n in range(1,50):\n",
    "    # We only need to extract the data once, which results has already done (for n = 10)\n",
    "    tRes = createPartitions(results, n)\n",
    "    # This saves a plot of the distribution as a histogram\n",
    "    plotHistogram(tRes, n)\n",
    "    \n",
    "    # Now we split into train and test\n",
    "    train, test = splitTrainTest(results)\n",
    "    \n",
    "    # Now we caculate the average RMSE\n",
    "    _, RMSE, _ = trainAndTest(train, test)\n",
    "    \n",
    "    rmses.append(sum(RMSE.values()) / len(RMSE))\n",
    "    max_rmses.append(max(RMSE.values()))\n",
    "    min_rmses.append(min(RMSE.values()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,50), rmses)\n",
    "plt.title(\"RMSE vs Resolution\")\n",
    "plt.xlabel(\"Geographic Resolution\")\n",
    "plt.ylabel(\"Average RMSE all regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
