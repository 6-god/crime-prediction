{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Notebook to Calculate Baseline using a simple average predictor.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions.\n",
    "'''\n",
    "# Mapping for data matrix columns.\n",
    "columns = { 'x' : 0,\n",
    "            'y' : 1,\n",
    "            'region' : 2,\n",
    "            't' : 3, \n",
    "            'count' : 4 } \n",
    "\n",
    "# Author: Alex Wang -- Sets NaN to average.\n",
    "def normalize_features(X_train):\n",
    "    mean_X_train = np.nanmean(X_train, 0)\n",
    "    for i in xrange(np.shape(X_train)[1]):\n",
    "        col = X_train[:,i]\n",
    "        col[ np.isnan(col) ] = mean_X_train[i]\n",
    "    std_X_train = np.std(X_train, 0)\n",
    "    std_X_train[ std_X_train == 0 ] = 1\n",
    "    X_train_normalized = (X_train - mean_X_train) / std_X_train\n",
    "    return X_train_normalized\n",
    "\n",
    "def rmse(predict, true):\n",
    "    # Returns the root mean squared error.\n",
    "    return np.sqrt(1.0/np.shape(predict)[0] * np.sum(np.square(predict - true)))\n",
    "\n",
    "def randomSplit(X, split_size):\n",
    "    # Randomly splits the data.\n",
    "    np.random.shuffle(X)\n",
    "    break_pt = int(split_size * np.shape(X)[0])\n",
    "    return X[:break_pt,:], X[break_pt:,:]\n",
    "\n",
    "def splitLastN(X, t):\n",
    "    # Splits the X data matrix into historical data and data for the \n",
    "    # last t time steps.\n",
    "    times = np.unique(X[:, columns['t']])\n",
    "    lowBound = np.sort(times)[len(times) - t]\n",
    "    selected = X[:, columns['t']] <= lowBound\n",
    "    return X[selected,:], X[~selected,:]\n",
    "\n",
    "def buckets(series, n):\n",
    "    # Takes a series and returns an array mapping each element to\n",
    "    # one of n buckets.\n",
    "    mi, ma = series.min(), series.max()\n",
    "    buckets = np.linspace(mi, ma, n + 1)\n",
    "    \n",
    "    res = np.zeros(len(series))\n",
    "    try:\n",
    "        array = series.values\n",
    "    except AttributeError:\n",
    "        array = series\n",
    "    if np.isnan(array).any():\n",
    "        print \"Error! NaN values found in series!\"\n",
    "    for i in xrange(n):\n",
    "        res[(buckets[i] <= array) & (array < buckets[i+1])] = i\n",
    "    return res.astype(int)\n",
    "\n",
    "def createSimplePartitions(data, n):\n",
    "    # Returns a partitioned version of data into nxn regions!\n",
    "    data['xRegion'] = buckets(data.Latitude, n).astype(int)\n",
    "    data['yRegion'] = buckets(data.Longitude, n).astype(int)\n",
    "    data['Region'] = n * data.xRegion + data.yRegion\n",
    "\n",
    "    return data\n",
    "\n",
    "    \n",
    "def extractRegionsFromMatrix(X_data, n):\n",
    "    # Does the same thing as extractDataMatrix, but as input takes in\n",
    "    # a matrix with just the latitude, longitude coordinates, time periods.\n",
    "    xRegion = buckets(X_data[:, 0], n)\n",
    "    yRegion = buckets(X_data[:, 1], n)\n",
    "    Region = n * xRegion + yRegion\n",
    "    \n",
    "    xRegions = np.unique(xRegion).astype(int)\n",
    "    yRegions = np.unique(yRegion).astype(int)\n",
    "    regions = np.unique(Region).astype(int)\n",
    "    months = np.unique(X_data[:,2])\n",
    "    num_columns = 5\n",
    "    num_rows = len(regions) * len(months)\n",
    "    X_res = np.zeros((num_rows, num_columns))\n",
    "    el = 0\n",
    "    for x in xRegions:\n",
    "        for y in yRegions:\n",
    "            for month in months:\n",
    "                reg = n * x + y\n",
    "                count = len(X_data[ (Region == reg) &\n",
    "                                    (X_data[:, 2] == month)])\n",
    "                if count > 0:\n",
    "                    X_res[el, :] = np.array([x,y,reg, month, count])\n",
    "                    el += 1\n",
    "            \n",
    "    # Convert data to right shape\n",
    "    X_res = X_res.astype(int)\n",
    "    if el < X_res.shape[0]:\n",
    "        print \"Removing empty values from our data!\"\n",
    "        print \"Rows before: {}\".format(X_res.shape[0])\n",
    "        X_res = X_res[~np.all(X_res == 0, axis=1)]\n",
    "        print \"Rows after: {}\".format(X_res.shape[0])\n",
    "    \n",
    "    return X_res\n",
    "\n",
    "def createDataMatrix(data):\n",
    "    '''\n",
    "    Transforms a panda dataframe into latitude longitude time period matrix\n",
    "    record of crimes.\n",
    "    '''\n",
    "    X = np.zeros((len(data), 3))\n",
    "    X[:, 0] = data.Latitude.values.astype(float)\n",
    "    X[:, 1] = data.Longitude.values.astype(float)\n",
    "    X[:, 2] = data.TimeFeature.values.astype(int)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def extractDataMatrix(data, n):\n",
    "    # Creates a NxD data matrix from the given data set.\n",
    "    # data must contains xRegion, yRegion, Region, and TimeFeature columns.\n",
    "    # 0 -> xRegion\n",
    "    # 1 -> yRegion\n",
    "    # 2 -> Region\n",
    "    # 3 -> Month\n",
    "    # 4 -> Count\n",
    "    # The data is NOT normalized!\n",
    "    # Returns the data as well as a dictionary mapping column names\n",
    "    # to indeces.\n",
    "    partData = createSimplePartitions(data, n)\n",
    "    regions = partData.Region.unique()\n",
    "    xRegions = partData.xRegion.unique()\n",
    "    yRegions = partData.yRegion.unique()\n",
    "    months = partData.TimeFeature.unique()\n",
    "    num_columns = 5\n",
    "    num_rows = len(regions) * len(months)\n",
    "    X_data = np.zeros((num_rows, num_columns))\n",
    "    el = 0\n",
    "    for x in xRegions:\n",
    "        for y in yRegions:\n",
    "            for month in months:\n",
    "                count = len(data[ (data.xRegion == x) &\n",
    "                                  (data.yRegion == y) &\n",
    "                                  (data.TimeFeature == month)])\n",
    "                if count > 0:\n",
    "                    X_data[el, :] = np.array([x,y,n*x + y, month, count])\n",
    "                    el += 1\n",
    "            \n",
    "    if el < X_data.shape[0]:\n",
    "        print \"Removing empty values from our data!\"\n",
    "        print \"Rows before: {}\".format(X_data.shape[0])\n",
    "        X_data = X_data[~np.all(X_data == 0, axis=1)]\n",
    "        print \"Rows after: {}\".format(X_data.shape[0])\n",
    "        \n",
    "    return X_data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "More utility functions. \n",
    "'''\n",
    "# Note: If a data point does not exist, it is assumed to be 0.\n",
    "def averagePredictions(X_train, nRegions, tMax):\n",
    "    averages = np.zeros(nRegions)\n",
    "    for region in xrange(nRegions):\n",
    "        averages[region] = X_train[\n",
    "            X_train[:, columns['region']] == region,\n",
    "            columns['count']].sum() / float(tMax)\n",
    "    return averages\n",
    "\n",
    "def createHeatMap(X):\n",
    "    '''\n",
    "    Given a data set, creates a heatmap of it based on x,y coordinates.\n",
    "    Ignore the temporal feature. You should subset the data before passing\n",
    "    it into this function if you'd like a heatmap for a specific time period.\n",
    "    '''\n",
    "    n = X[:, columns['x']].astype(int).max()\n",
    "    m = X[:, columns['y']].astype(int).max()\n",
    "    heatmap = np.zeros((n,m))\n",
    "    for i in xrange(n):\n",
    "        for j in xrange(m):\n",
    "            total = X[(X[:, columns['x']] == i) & \n",
    "                      (X[:, columns['y']] == j), columns['count']].sum()\n",
    "            if total > 0:\n",
    "                heatmap[i,j] = total\n",
    "                \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a value of n:\n",
    "# 0. Normalize the data (if set to True)\n",
    "# 1. Partition the data\n",
    "# 2. Split into Train/Test, where test has lastN time feats.\n",
    "#    Options: 'random', 'last'\n",
    "#    splitRatio specifies the ratio of results to keep for testing.\n",
    "#    testPeriods specifies the number of time periods to test\n",
    "# 3. Train the averages\n",
    "# 4. Test on the hold-out\n",
    "# 5. Calculate RMSE\n",
    "def averageModel(n, X_data, normalize = False, splitMethod = 'random',\n",
    "                 splitRatio = 0.2, testPeriods = 12, plot=None):\n",
    "    if normalize:\n",
    "        X_data = normalize_features(X_data)\n",
    "        print \"Normalized data features!\"\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    # Returns an array indexed by region with the average over the \n",
    "    # training set for each region.\n",
    "    tMax = X_data[:, columns['t']].max()\n",
    "    nRegions = X_data[:, columns['region']].max() + 1\n",
    "    \n",
    "    # Now we can split the data!\n",
    "    if splitMethod == 'random':\n",
    "        X_train, X_test = randomSplit(X_data, splitRatio)\n",
    "    elif splitMethod == 'last':\n",
    "        X_train, X_test = splitLastN(X_data, testPeriods)\n",
    "    else:\n",
    "        raise Exception(\"splitMethod {} unsupported\".format(splitMethod))\n",
    "    \n",
    "    print \"Training model...\"\n",
    "    sys.stdout.flush()\n",
    "    # Now use training data to calculate averages\n",
    "    model = averagePredictions(X_train, nRegions, tMax)\n",
    "    print \"Averages Model:\"\n",
    "    print model\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Generate predictions vector\n",
    "    predict = model[X_test[:, columns['region']]]\n",
    "    true = X_test[:, columns['count']]\n",
    "    \n",
    "    # Create distributions, and calculate the RMSE of the distribution\n",
    "    predict_dist = predict / float(np.sum(predict))\n",
    "    true_dist = true / float(np.sum(true))\n",
    "    \n",
    "    # Plot the figure if we're predicting on the end!\n",
    "    if plot is not None:\n",
    "        \n",
    "        # We want to plot the last 100 elements to see how this looks\n",
    "        minValue = min(len(predict_dist), 100)\n",
    "        yPred = predict_dist[-minValue:]\n",
    "        yTrue = true_dist[-minValue:]\n",
    "        plt.clf()\n",
    "        plt.plot(yPred, label=\"Predictions\")\n",
    "        plt.plot(yTrue, label=\"Actual Data\")\n",
    "        plt.title('Predictive Distribution')\n",
    "        plt.xlabel('Compressed Features')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.savefig('../figures/{}_results/n={}_periods={}.png'.format(\n",
    "            plot, n,testPeriods))\n",
    "        plt.legend()\n",
    "        plt.close()\n",
    "        \n",
    "        # Attach the predictions to the data\n",
    "        trueValues = np.copy(X_test).astype(float)\n",
    "        predictedValues = np.copy(X_test).astype(float)\n",
    "        trueValues[:, columns['count']]\n",
    "        predictedValues[:, columns['count']]\n",
    "        trueValues[:, columns['count']] = true_dist\n",
    "        predictedValues[:, columns['count']] = predict_dist\n",
    "        \n",
    "        # Now we want to plot the heatmaps for the predictions/actual data\n",
    "        # by time period\n",
    "        months = np.unique(X_test[: columns['region']])\n",
    "        for month in months:\n",
    "            # Create the heatmaps \n",
    "            selected = (X_test[:, columns['t']] == month)\n",
    "            if selected.sum() > 0:\n",
    "                plt.clf()\n",
    "                sns.heatmap(createHeatMap(trueValues[selected, :]))\n",
    "                plt.title('True Density Distribution in Month {}'.format(month))\n",
    "                plt.savefig('../figures/{}_results/heatmap_true_n={}_t={}.png'.format(\n",
    "                    plot, n, month))\n",
    "                plt.close()\n",
    "\n",
    "                plt.clf()\n",
    "                sns.heatmap(createHeatMap(predictedValues[selected, :]))\n",
    "                plt.title('Predicted Density Distribution in Month {}'.format(month))\n",
    "                plt.savefig('../figures/{}_results/heatmap_pred_n={}_t={}.png'.format(\n",
    "                    plot, n, month))\n",
    "                plt.close()\n",
    "        \n",
    "        \n",
    "    \n",
    "    print \"Calculating RMSE...\"\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return rmse(predict_dist, true_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's make a plot for some values of N to see if the data works out...\n",
    "sfdata_file = '../../cs281_data/large_data/sfclean.pk'\n",
    "with open(sfdata_file) as fp:\n",
    "    sfdata = pickle.load(fp)\n",
    "    # For sfdata, need to remove outliers\n",
    "    sfdata = sfdata[-120 > sfdata.Longitude][sfdata.Longitude > (-130)]\n",
    "    sfdata = sfdata[sfdata.Latitude > 37][sfdata.Latitude < 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 2\n",
      "CPU times: user 7.6 s, sys: 0 ns, total: 7.6 s\n",
      "Wall time: 7.6 s\n",
      "Partitioned data...\n",
      "Training model...\n",
      "Averages Model:\n",
      "[  270.35064935   667.69480519   170.41558442  1399.37012987]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 0.000237697841157\n",
      "Training model...\n",
      "Averages Model:\n",
      "[ 1157.28571429  2909.2987013    994.33766234  5939.02597403]\n",
      "Calculating RMSE...\n",
      "Last RMSE: 0.00290841107286\n",
      "n = 3\n",
      "CPU times: user 16.3 s, sys: 0 ns, total: 16.3 s\n",
      "Wall time: 16.3 s\n",
      "Partitioned data...\n",
      "Training model...\n",
      "Averages Model:\n",
      "[  65.87662338  218.42857143  197.24025974  131.94155844  470.33116883\n",
      "  427.52597403    8.73376623  378.21428571  508.43506494]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 0.0002110897339\n",
      "Training model...\n",
      "Averages Model:\n",
      "[  360.01948052   986.3961039    977.35714286   601.33116883  2663.23376623\n",
      "  2097.98051948    42.88961039  1399.98051948  1870.75974026]\n",
      "Calculating RMSE...\n",
      "Last RMSE: 0.00134040127748\n",
      "n = 4\n",
      "CPU times: user 28.1 s, sys: 0 ns, total: 28.1 s\n",
      "Wall time: 28 s\n",
      "Partitioned data...\n",
      "Training model...\n",
      "Averages Model:\n",
      "[   15.47402597   102.74025974   122.94155844   110.92857143    42.08441558\n",
      "    58.50649351   312.48051948   100.7012987     32.87012987   168.71428571\n",
      "  1296.55194805   131.18181818     0.            15.4025974    201.72727273\n",
      "    31.25324675]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 0.000165474283015\n",
      "Training model...\n",
      "Averages Model:\n",
      "[   96.38311688   511.43506494   637.09090909   528.36363636   283.38311688\n",
      "   266.08441558  1374.02597403   369.81818182   212.19480519   713.14935065\n",
      "  4534.64935065   462.98701299     0.            68.99350649   790.3961039\n",
      "   150.99350649]\n",
      "Calculating RMSE...\n",
      "Last RMSE: 0.0012409317211\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Create the data matrix for SanFrancisco\n",
    "X_data = createDataMatrix(sfdata)\n",
    "\n",
    "testN = range(2,10) + range(10,30,5)\n",
    "rmses = []\n",
    "for n in testN:\n",
    "    print \"n = {}\".format(n)\n",
    "    # Extracting more efficiently!\n",
    "    %time X_region = extractRegionsFromMatrix(X_data, n)\n",
    "    # print X_data.dtype\n",
    "    print \"Partitioned data...\"\n",
    "    sys.stdout.flush()\n",
    "    rmse_random = averageModel(n, X_region)\n",
    "    print \"Random RMSE: {}\".format(rmse_random)\n",
    "    sys.stdout.flush()\n",
    "    rmse_last = averageModel(n, X_region, plot='sf', splitMethod='last', testPeriods=12)\n",
    "    rmses.append((rmse_random, rmse_last))\n",
    "    print \"Last RMSE: {}\".format(rmse_last)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculat the RMSE for n = 15\n",
    "n = 15\n",
    "X_data = extractDataMatrix(sfdata, n)\n",
    "rmse_random15 = averageModel(n, X_data)\n",
    "rmse_last15 = averageModel(n, X_data, splitMethod='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(3251.3638018497254, 515.77474251496869),\n",
       "  (1447.171064391375, 215.41729545008857),\n",
       "  (1289.3316464751988, 167.83815175812575),\n",
       "  (809.42313953565497, 112.07265100677968),\n",
       "  (522.33102592997, 88.853815771321806),\n",
       "  (483.67467985515685, 85.632104612382335),\n",
       "  (411.730711190791, 70.962471993066828),\n",
       "  (301.01978243896173, 53.300374089523864),\n",
       "  (260.72621104254262, 57.288523402690132),\n",
       "  (133.00384412934449, 29.691957633717099),\n",
       "  (84.002157945116792, 25.657276412496376),\n",
       "  (42.734686731225423, 13.973661083494004),\n",
       "  (27.176216198750392, 9.7661827365748941),\n",
       "  (19.958992915729283, 7.5008050474014656),\n",
       "  (15.986312894185623, 6.0995323817263838)],\n",
       " 29.691957633717099,\n",
       " 133.00384412934449)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmses, rmse_last15, rmse_random15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmses = rmses[:9] + [(rmse_random15, rmse_last15)] + rmses[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a plot of the RMSEs that we calculated\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = testN[:len(rmses)]\n",
    "y1,y2 = zip(*rmses)\n",
    "line1 = plt.plot(x, y1, label=\"Final 12 Time Units\")\n",
    "line2 = plt.plot(x, y2, label=\"Random Sample Data\")\n",
    "plt.title('Baseline Predictions for San Francisco')\n",
    "plt.xlabel('Dimension of Grid')\n",
    "plt.ylabel('Root Mean Square Error')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
