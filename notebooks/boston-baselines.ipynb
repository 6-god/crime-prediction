{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Notebook to Calculate Baseline using a simple average predictor.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions.\n",
    "'''\n",
    "# Mapping for data matrix columns.\n",
    "columns = { 'x' : 0,\n",
    "            'y' : 1,\n",
    "            'region' : 2,\n",
    "            't' : 3, \n",
    "            'count' : 4 } \n",
    "\n",
    "# Author: Alex Wang -- Sets NaN to average.\n",
    "def normalize_features(X_train):\n",
    "    mean_X_train = np.nanmean(X_train, 0)\n",
    "    for i in xrange(np.shape(X_train)[1]):\n",
    "        col = X_train[:,i]\n",
    "        col[ np.isnan(col) ] = mean_X_train[i]\n",
    "    std_X_train = np.std(X_train, 0)\n",
    "    std_X_train[ std_X_train == 0 ] = 1\n",
    "    X_train_normalized = (X_train - mean_X_train) / std_X_train\n",
    "    return X_train_normalized\n",
    "\n",
    "def rmse(predict, true):\n",
    "    # Returns the root mean squared error.\n",
    "    return np.sqrt(1.0/np.shape(predict)[0] * np.sum(np.square(predict - true)))\n",
    "\n",
    "def randomSplit(X, split_size):\n",
    "    # Randomly splits the data.\n",
    "    np.random.shuffle(X)\n",
    "    break_pt = int(split_size * np.shape(X)[0])\n",
    "    return X[:break_pt,:], X[break_pt:,:]\n",
    "\n",
    "def splitLastN(X, t):\n",
    "    # Splits the X data matrix into historical data and data for the \n",
    "    # last t time steps.\n",
    "    times = np.unique(X[:, columns['t']])\n",
    "    lowBound = np.sort(times)[len(times) - t]\n",
    "    selected = X[:, columns['t']] <= lowBound\n",
    "    return X[selected,:], X[~selected,:]\n",
    "\n",
    "def buckets(series, n):\n",
    "    # Takes a series and returns an array mapping each element to\n",
    "    # one of n buckets.\n",
    "    mi, ma = series.min(), series.max()\n",
    "    buckets = np.linspace(mi, ma, n + 1)\n",
    "    \n",
    "    res = np.zeros(len(series))\n",
    "    try:\n",
    "        array = series.values\n",
    "    except AttributeError:\n",
    "        array = series\n",
    "    if np.isnan(array).any():\n",
    "        print \"Error! NaN values found in series!\"\n",
    "    for i in xrange(n):\n",
    "        res[(buckets[i] <= array) & (array < buckets[i+1])] = i\n",
    "    return res.astype(int)\n",
    "\n",
    "def createSimplePartitions(data, n):\n",
    "    # Returns a partitioned version of data into nxn regions!\n",
    "    data['xRegion'] = buckets(data.Latitude, n).astype(int)\n",
    "    data['yRegion'] = buckets(data.Longitude, n).astype(int)\n",
    "    data['Region'] = n * data.xRegion + data.yRegion\n",
    "\n",
    "    return data\n",
    "\n",
    "    \n",
    "def extractRegionsFromMatrix(X_data, n):\n",
    "    # Does the same thing as extractDataMatrix, but as input takes in\n",
    "    # a matrix with just the latitude, longitude coordinates, time periods.\n",
    "    xRegion = buckets(X_data[:, 0], n)\n",
    "    yRegion = buckets(X_data[:, 1], n)\n",
    "    Region = n * xRegion + yRegion\n",
    "    \n",
    "    xRegions = np.unique(xRegion).astype(int)\n",
    "    yRegions = np.unique(yRegion).astype(int)\n",
    "    regions = np.unique(Region).astype(int)\n",
    "    months = np.unique(X_data[:,2])\n",
    "    num_columns = 5\n",
    "    num_rows = len(regions) * len(months)\n",
    "    X_res = np.zeros((num_rows, num_columns))\n",
    "    el = 0\n",
    "    for x in xRegions:\n",
    "        for y in yRegions:\n",
    "            for month in months:\n",
    "                reg = n * x + y\n",
    "                count = len(X_data[ (Region == reg) &\n",
    "                                    (X_data[:, 2] == month)])\n",
    "                if count > 0:\n",
    "                    X_res[el, :] = np.array([x,y,reg, month, count])\n",
    "                    el += 1\n",
    "            \n",
    "    # Convert data to right shape\n",
    "    X_res = X_res.astype(int)\n",
    "    if el < X_res.shape[0]:\n",
    "        print \"Removing empty values from our data!\"\n",
    "        print \"Rows before: {}\".format(X_res.shape[0])\n",
    "        X_res = X_res[~np.all(X_res == 0, axis=1)]\n",
    "        print \"Rows after: {}\".format(X_res.shape[0])\n",
    "    \n",
    "    return X_res\n",
    "\n",
    "def createDataMatrix(data):\n",
    "    '''\n",
    "    Transforms a panda dataframe into latitude longitude time period matrix\n",
    "    record of crimes.\n",
    "    '''\n",
    "    X = np.zeros((len(data), 3))\n",
    "    X[:, 0] = data.Latitude.values.astype(float)\n",
    "    X[:, 1] = data.Longitude.values.astype(float)\n",
    "    X[:, 2] = data.TimeFeature.values.astype(int)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def extractDataMatrix(data, n):\n",
    "    # Creates a NxD data matrix from the given data set.\n",
    "    # data must contains xRegion, yRegion, Region, and TimeFeature columns.\n",
    "    # 0 -> xRegion\n",
    "    # 1 -> yRegion\n",
    "    # 2 -> Region\n",
    "    # 3 -> Month\n",
    "    # 4 -> Count\n",
    "    # The data is NOT normalized!\n",
    "    # Returns the data as well as a dictionary mapping column names\n",
    "    # to indeces.\n",
    "    partData = createSimplePartitions(data, n)\n",
    "    regions = partData.Region.unique()\n",
    "    xRegions = partData.xRegion.unique()\n",
    "    yRegions = partData.yRegion.unique()\n",
    "    months = partData.TimeFeature.unique()\n",
    "    num_columns = 5\n",
    "    num_rows = len(regions) * len(months)\n",
    "    X_data = np.zeros((num_rows, num_columns))\n",
    "    el = 0\n",
    "    for x in xRegions:\n",
    "        for y in yRegions:\n",
    "            for month in months:\n",
    "                count = len(data[ (data.xRegion == x) &\n",
    "                                  (data.yRegion == y) &\n",
    "                                  (data.TimeFeature == month)])\n",
    "                if count > 0:\n",
    "                    X_data[el, :] = np.array([x,y,n*x + y, month, count])\n",
    "                    el += 1\n",
    "            \n",
    "    if el < X_data.shape[0]:\n",
    "        print \"Removing empty values from our data!\"\n",
    "        print \"Rows before: {}\".format(X_data.shape[0])\n",
    "        X_data = X_data[~np.all(X_data == 0, axis=1)]\n",
    "        print \"Rows after: {}\".format(X_data.shape[0])\n",
    "        \n",
    "    return X_data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "More utility functions. \n",
    "'''\n",
    "# Note: If a data point does not exist, it is assumed to be 0.\n",
    "def averagePredictions(X_train, nRegions, tMax):\n",
    "    averages = np.zeros(nRegions)\n",
    "    for region in xrange(nRegions):\n",
    "        averages[region] = X_train[\n",
    "            X_train[:, columns['region']] == region,\n",
    "            columns['count']].sum() / float(tMax)\n",
    "    return averages\n",
    "\n",
    "def createHeatMap(X):\n",
    "    '''\n",
    "    Given a data set, creates a heatmap of it based on x,y coordinates.\n",
    "    Ignore the temporal feature. You should subset the data before passing\n",
    "    it into this function if you'd like a heatmap for a specific time period.\n",
    "    '''\n",
    "    n = X[:, columns['x']].astype(int).max()\n",
    "    m = X[:, columns['y']].astype(int).max()\n",
    "    heatmap = np.zeros((n,m))\n",
    "    for i in xrange(n):\n",
    "        for j in xrange(m):\n",
    "            total = X[(X[:, columns['x']] == i) & \n",
    "                      (X[:, columns['y']] == j), columns['count']].sum()\n",
    "            if total > 0:\n",
    "                heatmap[i,j] = total\n",
    "    \n",
    "    # Normalize\n",
    "    heatmap = heatmap / float(heatmap.sum())\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make some plots for n = 15 for GP process\n",
    "def plotDistribution(predict, true, city, n, process='GP'):\n",
    "    minValue = min(len(predict), 100)\n",
    "    yPred = predict[-minValue:]\n",
    "    yTrue = true[-minValue:]\n",
    "    yPred = yPred / float(np.sum(yPred))\n",
    "    yTrue = yTrue / float(np.sum(yTrue))\n",
    "    plt.clf()\n",
    "    plt.plot(yPred, label=\"Predictions\")\n",
    "    plt.plot(yTrue, label=\"Actual Data\")\n",
    "    plt.title('Predictive Distribution for {}'.format(process))\n",
    "    plt.xlabel('Compressed Features')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    plt.savefig('../figures/{}_results/{}_n={}.png'.format(\n",
    "        city, process, n))\n",
    "    plt.close()\n",
    "    \n",
    "def plotHeatMaps(X_test, predict, city, n, process='GP'):\n",
    "    # Attach the predictions to the data\n",
    "    trueValues = np.copy(X_test)\n",
    "    predictedValues = np.copy(X_test)\n",
    "    predictedValues[:, columns['count']] = predict\n",
    "\n",
    "    # Now we want to plot the heatmaps for the predictions/actual data\n",
    "    # by time period\n",
    "    months = np.unique(X_test[:, columns['t']])\n",
    "    for month in months:\n",
    "        # Create the heatmaps \n",
    "        selected = (X_test[:, columns['t']] == month)\n",
    "        if selected.sum() > 0:\n",
    "            plt.clf()\n",
    "            m = createHeatMap(trueValues[selected, :])\n",
    "            if m.sum() > 0:\n",
    "                sns.heatmap(m)\n",
    "                plt.title('True Density Distribution in Month {}'.format(month))\n",
    "                plt.savefig('../figures/{}_results/{}_heatmap_true_n={}_t={}.png'.format(\n",
    "                    city, process, n, month))\n",
    "                plt.close()\n",
    "\n",
    "            plt.clf()\n",
    "            m = createHeatMap(predictedValues[selected, :])\n",
    "            if m.sum() > 0:\n",
    "                sns.heatmap(m)\n",
    "                plt.title('Predicted Density Distribution in Month {}'.format(month))\n",
    "                plt.savefig('../figures/{}_results/{}_heatmap_pred_n={}_t={}.png'.format(\n",
    "                    city, process, n, month))\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a plot of the RMSEs that we calculated\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's make a plot for some values of N to see if the data works out...\n",
    "data_file = '../../cs281_data/large_data/bosclean.pk'\n",
    "with open(data_file) as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a value of n:\n",
    "# 0. Normalize the data (if set to True)\n",
    "# 1. Partition the data\n",
    "# 2. Split into Train/Test, where test has lastN time feats.\n",
    "#    Options: 'random', 'last'\n",
    "#    splitRatio specifies the ratio of results to keep for testing.\n",
    "#    testPeriods specifies the number of time periods to test\n",
    "# 3. Train the averages\n",
    "# 4. Test on the hold-out\n",
    "# 5. Calculate RMSE\n",
    "def averageModel(n, X_data, normalize = False, splitMethod = 'random',\n",
    "                 splitRatio = 0.2, testPeriods = 12, plot=None):\n",
    "    if normalize:\n",
    "        X_data = normalize_features(X_data)\n",
    "        print \"Normalized data features!\"\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    # Returns an array indexed by region with the average over the \n",
    "    # training set for each region.\n",
    "    tMax = X_data[:, columns['t']].max()\n",
    "    nRegions = X_data[:, columns['region']].max() + 1\n",
    "    \n",
    "    # Now we can split the data!\n",
    "    if splitMethod == 'random':\n",
    "        X_train, X_test = randomSplit(X_data, splitRatio)\n",
    "    elif splitMethod == 'last':\n",
    "        X_train, X_test = splitLastN(X_data, testPeriods)\n",
    "    else:\n",
    "        raise Exception(\"splitMethod {} unsupported\".format(splitMethod))\n",
    "    \n",
    "    print \"Training model...\"\n",
    "    sys.stdout.flush()\n",
    "    # Now use training data to calculate averages\n",
    "    model = averagePredictions(X_train, nRegions, tMax)\n",
    "    print \"Averages Model:\"\n",
    "    print model\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Generate predictions vector\n",
    "    predict = model[X_test[:, columns['region']]]\n",
    "    true = X_test[:, columns['count']]\n",
    "    \n",
    "    # Create distributions, and calculate the RMSE of the distribution\n",
    "    predict_dist = predict / float(np.sum(predict))\n",
    "    true_dist = true / float(np.sum(true))\n",
    "    \n",
    "    # Plot the figure if we're predicting on the end!\n",
    "    if plot is not None:\n",
    "        \n",
    "        plotDistribution(predict, true, 'boston', n, process='Baseline')\n",
    "        \n",
    "        plotHeatMaps(X_test, predict, 'boston', n, process='Baseline')\n",
    "            \n",
    "    print \"Calculating RMSE...\"\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return rmse(predict_dist, true_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 2\n",
      "CPU times: user 408 ms, sys: 0 ns, total: 408 ms\n",
      "Wall time: 410 ms\n",
      "Partitioned data...\n",
      "Training model...\n",
      "Averages Model:\n",
      "[ 398.11627907  648.76744186   54.81395349  179.34883721]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 0.00442310531153\n",
      "Training model...\n",
      "Averages Model:\n",
      "[ 1188.86046512  1507.90697674   479.81395349  1288.04651163]\n",
      "Calculating RMSE...\n",
      "Last RMSE: 0.0064390252893\n",
      "n = 3\n",
      "Removing empty values from our data!\n",
      "Rows before: 342\n",
      "Rows after: 305\n",
      "CPU times: user 876 ms, sys: 12 ms, total: 888 ms\n",
      "Wall time: 1.14 s\n",
      "Partitioned data...\n",
      "Training model...\n",
      "Averages Model:\n",
      "[  61.58139535   76.48837209   81.69767442   90.20930233  568.39534884\n",
      "  285.04651163    0.            6.1627907    33.93023256]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 0.00103329800133\n",
      "Training model...\n",
      "Averages Model:\n",
      "[  360.8372093    243.6744186    290.02325581   332.69767442  1980.11627907\n",
      "  1115.51162791     0.            31.76744186   110.        ]\n",
      "Calculating RMSE...\n",
      "Last RMSE: 0.00472936968399\n",
      "n = 4\n",
      "Removing empty values from our data!\n",
      "Rows before: 570\n",
      "Rows after: 525\n",
      "CPU times: user 1.44 s, sys: 4 ms, total: 1.44 s\n",
      "Wall time: 1.5 s\n",
      "Partitioned data...\n",
      "Training model...\n",
      "Averages Model:\n",
      "[  3.90465116e+01   3.37209302e+01   8.59767442e+01   1.44418605e+01\n",
      "   4.53255814e+01   1.57627907e+02   4.39767442e+02   1.53488372e+00\n",
      "   2.90697674e+00   1.36860465e+02   1.60325581e+02   1.07302326e+02\n",
      "   1.16279070e-01   9.30232558e-02   0.00000000e+00   6.37209302e+00]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 0.00126719070051\n",
      "Training model...\n",
      "Averages Model:\n",
      "[   94.76744186   142.48837209   270.23255814    41.53488372   183.88372093\n",
      "   767.72093023  1180.18604651    15.95348837     8.51162791   468.53488372\n",
      "   714.25581395   536.             0.             2.76744186     0.\n",
      "    37.79069767]\n",
      "Calculating RMSE...\n",
      "Last RMSE: 0.00275655504354\n",
      "n = 5\n",
      "Removing empty values from our data!\n",
      "Rows before: 874\n",
      "Rows after: 731\n",
      "CPU times: user 1.72 s, sys: 0 ns, total: 1.72 s\n",
      "Wall time: 1.85 s\n",
      "Partitioned data...\n",
      "Training model...\n",
      "Averages Model:\n",
      "[  7.67441860e+00   2.28837209e+01   4.18604651e-01   4.30465116e+01\n",
      "   6.74418605e-01   2.42558140e+01   8.14186047e+01   4.62790698e+01\n",
      "   7.39069767e+01   1.39534884e-01   0.00000000e+00   9.22093023e+01\n",
      "   3.40186047e+02   2.33790698e+02   4.31627907e+01   0.00000000e+00\n",
      "   6.37209302e+00   1.10000000e+01   2.27441860e+01   6.61860465e+01\n",
      "   3.72093023e-01   0.00000000e+00   5.34883721e-01   0.00000000e+00\n",
      "   5.46511628e+00]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 0.000996306013154\n",
      "Training model...\n",
      "Averages Model:\n",
      "[  3.39302326e+01   1.08302326e+02   2.02325581e+00   2.18790698e+02\n",
      "   9.76744186e-01   1.36604651e+02   2.48186047e+02   3.00697674e+02\n",
      "   2.86209302e+02   6.51162791e-01   0.00000000e+00   5.09581395e+02\n",
      "   9.67534884e+02   1.11695349e+03   1.45581395e+02   0.00000000e+00\n",
      "   2.74418605e+01   3.01162791e+01   1.16558140e+02   1.98674419e+02\n",
      "   0.00000000e+00   0.00000000e+00   2.76744186e+00   0.00000000e+00\n",
      "   1.30465116e+01]\n",
      "Calculating RMSE...\n",
      "Last RMSE: 0.00223695118141\n",
      "n = 6\n",
      "Removing empty values from our data!\n",
      "Rows before: 1178\n",
      "Rows after: 946\n",
      "CPU times: user 2.98 s, sys: 4 ms, total: 2.98 s\n",
      "Wall time: 3.12 s\n",
      "Partitioned data...\n",
      "Training model...\n",
      "Averages Model:\n",
      "[  4.32558140e+00   7.90697674e+00   4.58139535e+00   1.16744186e+01\n",
      "   3.33488372e+01   2.09302326e-01   2.99534884e+01   4.67441860e+01\n",
      "   4.68372093e+01   9.93023256e+00   2.45581395e+01   4.18604651e-01\n",
      "   1.16279070e-01   3.74883721e+01   1.06255814e+02   5.98139535e+01\n",
      "   1.12790698e+02   2.34883721e+00   0.00000000e+00   3.00697674e+01\n",
      "   1.60720930e+02   1.40162791e+02   1.82534884e+02   6.10232558e+01\n",
      "   1.39534884e-01   0.00000000e+00   0.00000000e+00   5.09302326e+00\n",
      "   5.41860465e+00   3.14418605e+01   0.00000000e+00   0.00000000e+00\n",
      "   3.95348837e-01   0.00000000e+00   0.00000000e+00   4.18604651e-01]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 0.00102967806395\n",
      "Training model...\n",
      "Averages Model:\n",
      "[  14.81395349   78.65116279   16.79069767   40.65116279  115.86046512\n",
      "    0.          108.30232558  159.06976744  118.34883721   67.88372093\n",
      "  174.1627907     0.            0.          246.          446.88372093\n",
      "  732.58139535  366.86046512    9.90697674    0.           86.69767442\n",
      "  390.34883721  410.30232558  572.62790698  166.11627907    0.            0.\n",
      "    0.           29.           11.34883721   96.69767442    0.            0.\n",
      "    2.76744186    0.            0.            1.95348837]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Create the data matrix for Boston\n",
    "X_data = createDataMatrix(data)\n",
    "\n",
    "testN = range(2,10) + range(10,30,5)\n",
    "rmses = []\n",
    "for n in testN:\n",
    "    print \"n = {}\".format(n)\n",
    "    # Extracting more efficiently!\n",
    "    %time X_region = extractRegionsFromMatrix(X_data, n)\n",
    "    # print X_data.dtype\n",
    "    print \"Partitioned data...\"\n",
    "    sys.stdout.flush()\n",
    "    rmse_random = averageModel(n, X_region)\n",
    "    print \"Random RMSE: {}\".format(rmse_random)\n",
    "    sys.stdout.flush()\n",
    "    rmse_last = averageModel(n, X_region, plot='boston', splitMethod='last', testPeriods=12)\n",
    "    rmses.append((rmse_random, rmse_last))\n",
    "    print \"Last RMSE: {}\".format(rmse_last)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = testN[:len(rmses)]\n",
    "y1,y2 = zip(*rmses)\n",
    "line1 = plt.plot(x, y1, label=\"Final Time Units\")\n",
    "line2 = plt.plot(x, y2, label=\"Random Sample Data\")\n",
    "plt.title('Baseline Predictions for Boston')\n",
    "plt.xlabel('Dimension of Grid')\n",
    "plt.ylabel('Root Mean Square Error of Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
