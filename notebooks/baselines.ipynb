{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Notebook to Calculate Baseline using a simple average predictor.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions.\n",
    "'''\n",
    "# Mapping for data matrix columns.\n",
    "columns = { 'x' : 0,\n",
    "            'y' : 1,\n",
    "            'region' : 2,\n",
    "            't' : 3, \n",
    "            'count' : 4 } \n",
    "\n",
    "# Author: Alex Wang -- Sets NaN to average.\n",
    "def normalize_features(X_train):\n",
    "    mean_X_train = np.nanmean(X_train, 0)\n",
    "    for i in xrange(np.shape(X_train)[1]):\n",
    "        col = X_train[:,i]\n",
    "        col[ np.isnan(col) ] = mean_X_train[i]\n",
    "    std_X_train = np.std(X_train, 0)\n",
    "    std_X_train[ std_X_train == 0 ] = 1\n",
    "    X_train_normalized = (X_train - mean_X_train) / std_X_train\n",
    "    return X_train_normalized\n",
    "\n",
    "def rmse(predict, true):\n",
    "    # Returns the root mean squared error.\n",
    "    return np.sqrt(1.0/np.shape(predict)[0] * np.sum(np.square(predict - true)))\n",
    "\n",
    "def randomSplit(X, split_size):\n",
    "    # Randomly splits the data.\n",
    "    np.random.shuffle(X)\n",
    "    break_pt = int(split_size * np.shape(X)[0])\n",
    "    return X[:break_pt,:], X[break_pt:,:]\n",
    "\n",
    "def splitLastN(X, t):\n",
    "    # Splits the X data matrix into historical data and data for the \n",
    "    # last t time steps.\n",
    "    times = np.unique(X[:, columns['t']])\n",
    "    lowBound = np.sort(times)[len(times) - t]\n",
    "    selected = X[:, columns['t']] <= lowBound\n",
    "    return X[selected,:], X[~selected,:]\n",
    "\n",
    "def buckets(series, n):\n",
    "    # Takes a series and returns an array mapping each element to\n",
    "    # one of n buckets.\n",
    "    mi, ma = series.min(), series.max()\n",
    "    buckets = np.linspace(mi, ma, n + 1)\n",
    "    \n",
    "    res = np.zeros(len(series))\n",
    "    array = series.values\n",
    "    if np.isnan(array).any():\n",
    "        print \"Error! NaN values found in series!\"\n",
    "    for i in xrange(n):\n",
    "        res[(buckets[i] <= array) & (array < buckets[i+1])] = i\n",
    "    return res.astype(int)\n",
    "\n",
    "def createSimplePartitions(data, n):\n",
    "    # Returns a partitioned version of data into nxn regions!\n",
    "    data['xRegion'] = buckets(data.Latitude, n).astype(int)\n",
    "    data['yRegion'] = buckets(data.Longitude, n).astype(int)\n",
    "    data['Region'] = n * data.xRegion + data.yRegion\n",
    "\n",
    "    return data\n",
    "\n",
    "def extractDataMatrix(data, n):\n",
    "    # Creates a NxD data matrix from the given data set.\n",
    "    # data must contains xRegion, yRegion, Region, and TimeFeature columns.\n",
    "    # 0 -> xRegion\n",
    "    # 1 -> yRegion\n",
    "    # 2 -> Region\n",
    "    # 3 -> Month\n",
    "    # 4 -> Count\n",
    "    # The data is NOT normalized!\n",
    "    # Returns the data as well as a dictionary mapping column names\n",
    "    # to indeces.\n",
    "    partData = createSimplePartitions(data, n)\n",
    "    regions = partData.Region.unique()\n",
    "    months = partData.TimeFeature.unique()\n",
    "    num_columns = 5\n",
    "    num_rows = len(regions) * len(months)\n",
    "    X_data = np.zeros((num_rows, num_columns))\n",
    "    el = 0\n",
    "    for region in regions:\n",
    "        for month in months:\n",
    "            tmp = data[ (data.Region == region) &\n",
    "                        (data.TimeFeature == month)]\n",
    "            # print tmp\n",
    "            count = len(tmp)\n",
    "            if count > 0:\n",
    "                X_data[el, :] = np.array([tmp.xRegion.iloc[0],\n",
    "                                            tmp.yRegion.iloc[0],\n",
    "                                            region, month, count])\n",
    "                el += 1\n",
    "            \n",
    "    if el < X_data.shape[0]:\n",
    "        print \"Removing empty values from our data!\"\n",
    "        print \"Rows before: {}\".format(X_data.shape[0])\n",
    "        X_data = X_data[~np.all(X_data == 0, axis=1)]\n",
    "        print \"Rows after: {}\".format(X_data.shape[0])\n",
    "        \n",
    "    return X_data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "More utility functions. \n",
    "'''\n",
    "# Note: If a data point does not exist, it is assumed to be 0.\n",
    "def averagePredictions(X_train):\n",
    "    # Returns an array indexed by region with the average over the \n",
    "    # training set for each region.\n",
    "    tMax = X_train[:, columns['t']].max()\n",
    "    nRegions = X_train[:, columns['region']].max() + 1\n",
    "    \n",
    "    averages = np.zeros(nRegions)\n",
    "    for region in xrange(nRegions):\n",
    "        averages[region] = X_train[\n",
    "            X_train[:, columns['region']] == region,\n",
    "            columns['count']].sum() / float(tMax)\n",
    "    return averages\n",
    "\n",
    "def createHeatMap(X):\n",
    "    '''\n",
    "    Given a data set, creates a heatmap of it based on x,y coordinates.\n",
    "    Ignore the temporal feature. You should subset the data before passing\n",
    "    it into this function if you'd like a heatmap for a specific time period.\n",
    "    '''\n",
    "    n = X[:, columns['x']].max()\n",
    "    m = X[:, columns['y']].max()\n",
    "    heatmap = np.zeros((n,m))\n",
    "    for i in xrange(n):\n",
    "        for j in xrange(m):\n",
    "            total = X[:, (X[:, columns['x']] == i) & \n",
    "                         (X[:, columns['y']] == j)].sum()\n",
    "            if total > 0:\n",
    "                heatmap[i,j] = total\n",
    "                \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a value of n:\n",
    "# 0. Normalize the data (if set to True)\n",
    "# 1. Partition the data\n",
    "# 2. Split into Train/Test, where test has lastN time feats.\n",
    "#    Options: 'random', 'last'\n",
    "#    splitRatio specifies the ratio of results to keep for testing.\n",
    "#    testPeriods specifies the number of time periods to test\n",
    "# 3. Train the averages\n",
    "# 4. Test on the hold-out\n",
    "# 5. Calculate RMSE\n",
    "def averageModel(n, X_data, normalize = False, splitMethod = 'random', splitRatio = 0.1, testPeriods = 12):\n",
    "    if normalize:\n",
    "        X_data = normalize_features(X_data)\n",
    "        print \"Normalized data features!\"\n",
    "        sys.stdout.flush()\n",
    "    if splitMethod == 'random':\n",
    "        X_train, X_test = randomSplit(X_data, splitRatio)\n",
    "    elif splitMethod == 'last':\n",
    "        X_train, X_test = splitLastN(X_data, testPeriods)\n",
    "    else:\n",
    "        raise Exception(\"splitMethod {} unsupported\".format(splitMethod))\n",
    "    \n",
    "    print \"Training model...\"\n",
    "    sys.stdout.flush()\n",
    "    # Now use training data to calculate averages\n",
    "    model = averagePredictions(X_train)\n",
    "    print model\n",
    "    \n",
    "    # Generate predictions vector\n",
    "    predict = model[X_test[:, columns['region']]]\n",
    "    true = X_test[:, columns['count']]\n",
    "    \n",
    "    print \"Calculating RMSE...\"\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return rmse(predict, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's make a plot for some values of N to see if the data works out...\n",
    "sfdata_file = '../../cs281_data/large_data/sfclean.pk'\n",
    "with open(sfdata_file) as fp:\n",
    "    sfdata = pickle.load(fp)\n",
    "    # For sfdata, need to remove outliers\n",
    "    sfdata = sfdata[-120 > sfdata.Longitude][sfdata.Longitude > (-130)]\n",
    "    sfdata = sfdata[sfdata.Latitude > 37][sfdata.Latitude < 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 2\n",
      "Partitioned data...\n",
      "Training model...\n",
      "[ 115.30519481  259.73376623  119.20779221  771.88311688]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 3251.36380185\n",
      "Training model...\n",
      "[ 1246.30769231  3133.09090909  1070.82517483  6395.87412587]\n",
      "Calculating RMSE...\n",
      "Last RMSE: 515.774742515\n",
      "n = 3\n",
      "Partitioned data...\n",
      "Training model...\n",
      "[  39.70198675   89.94039735  100.84768212   85.28476821  358.05298013\n",
      "  196.98675497    6.74172185   67.94701987  171.62251656]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 1447.17106439\n",
      "Training model...\n",
      "[  387.71328671  1062.27272727  1052.53846154   647.58741259  2868.0979021\n",
      "  2259.36363636    46.18881119  1507.67132867  2014.66433566]\n",
      "Calculating RMSE...\n",
      "Last RMSE: 215.41729545\n",
      "n = 4\n",
      "Partitioned data...\n",
      "Training model...\n",
      "[   8.9025974    64.07142857   39.81168831   63.           40.35064935\n",
      "   32.03896104  164.33116883   33.80519481   13.44155844  100.99350649\n",
      "  407.71428571   62.63636364    0.            8.12987013   84.61038961\n",
      "   17.85064935]\n",
      "Calculating RMSE...\n",
      "Random RMSE: 1289.33164648\n",
      "Training model...\n",
      "[  103.7972028    550.77622378   686.0979021    569.00699301   305.18181818\n",
      "   286.55244755  1479.72027972   398.26573427   228.51748252   768.00699301\n",
      "  4883.46853147   498.6013986      0.            74.3006993    851.1958042\n",
      "   162.60839161]\n",
      "Calculating RMSE...\n",
      "Last RMSE: 167.838151758\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "testN = range(2,10) + range(10,100,10)\n",
    "rmses = []\n",
    "for n in testN:\n",
    "    print \"n = {}\".format(n)\n",
    "    X_data = extractDataMatrix(sfdata, n)\n",
    "    # print X_data.dtype\n",
    "    print \"Partitioned data...\"\n",
    "    sys.stdout.flush()\n",
    "    rmse_random = averageModel(n, X_data)\n",
    "    print \"Random RMSE: {}\".format(rmse_random)\n",
    "    sys.stdout.flush()\n",
    "    rmse_last = averageModel(n, X_data, splitMethod='last')\n",
    "    rmses.append((rmse_random, rmse_last))\n",
    "    print \"Last RMSE: {}\".format(rmse_last)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
